{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yug\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# works best with len(samples) = 2, 4, 6\n",
    "def show_samples(samples):\n",
    "    y = int(len(samples) / 2)\n",
    "    for i in range(len(samples)):\n",
    "        plt.subplot(2, y, i + 1)\n",
    "        plt.subplots_adjust(hspace=1, wspace=1)\n",
    "        plt.imshow(samples[i][0])\n",
    "        plt.title(\"Uninfected\" if samples[i][1] else \"Infected\", loc='center')\n",
    "        plt.ylabel(f\"Height ({samples[i][0].shape[0]})\")\n",
    "        plt.xlabel(f\"Width ({samples[i][0].shape[1]})\")\n",
    "    plt.show()\n",
    "\n",
    "class MalariaDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataframe[index]\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "def get_loader(df, batch_size=32, transform=None): \n",
    "    dataset = MalariaDataset(df, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def evaluate(model, loader): \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images) \n",
    "        total_correct += int(((outputs > 0.0).squeeze().long() == labels).sum())\n",
    "        total_samples += len(labels)\n",
    "    return float(total_correct) / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "data_dir = f\"../data/corrected_{input_size}.pickle\"\n",
    "df = pd.read_pickle(data_dir)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "df[\"image\"] = df[\"image\"].apply(lambda x: transform(x))\n",
    "\n",
    "np_df = df.to_numpy()\n",
    "np.random.seed(1000)\n",
    "np.random.shuffle(np_df)\n",
    "\n",
    "num_samples = len(np_df)\n",
    "a = int(num_samples * 0.8)\n",
    "b = int(num_samples * 0.9)\n",
    "df_trn = np_df[:a]\n",
    "df_val = np_df[a:b]\n",
    "df_tst = np_df[b:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trn_transform = transforms.Compose([\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.25),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05)),\n",
    "    transforms.RandomRotation(180), \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "])\n",
    "\n",
    "trn_loader = get_loader(df_trn, batch_size=batch_size, transform=trn_transform)\n",
    "val_loader = get_loader(df_val, batch_size=batch_size)\n",
    "tst_loader = get_loader(df_tst, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "alexnet = torchvision.models.alexnet(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: os.mkdir(\"./alexnet_features\")\n",
    "except: assert os.path.isdir(\"./alexnet_features\")\n",
    "\n",
    "def compute_features(name, loader): \n",
    "    features = []\n",
    "    for images, _ in loader: \n",
    "        images = images.to(device)\n",
    "        fts = alexnet.features(images)\n",
    "        features.append(fts.cpu().detach().numpy())\n",
    "    with open(f\"./alexnet_features/{name}.pkl\", 'wb') as f:\n",
    "        pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_features(\"trn\", trn_loader)\n",
    "compute_features(\"val\", val_loader)\n",
    "compute_features(\"tst\", tst_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 1), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(128, 64, 1), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 16, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(16 * 6 * 6, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300 \n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Classifier().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "try: os.mkdir(\"./alexnet_models\")\n",
    "except: assert os.path.isdir(\"./alexnet_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_features = pickle.load(open(f\"./alexnet_features/trn.pkl\", 'rb'))\n",
    "# val_features = pickle.load(open(f\"./alexnet_features/val.pkl\", 'rb'))\n",
    "\n",
    "best = {\n",
    "    \"epoch\": 0, \n",
    "    \"accuracy\": 0\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for i, (images, labels) in enumerate(trn_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.unsqueeze(1).float()\n",
    "        labels = labels.to(device)\n",
    "        # forward pass\n",
    "        features = alexnet.features(images)\n",
    "        outputs = model(features)\n",
    "        # compute loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # training accuracy \n",
    "        total_correct += int(((outputs > 0.0) == labels).sum())\n",
    "        total_samples += len(labels)\n",
    "    training_accuracy = float(total_correct) / total_samples\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for i, (images, labels) in enumerate(val_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.unsqueeze(1).float()\n",
    "        labels = labels.to(device)\n",
    "        features = alexnet.features(images)\n",
    "        outputs = model(features)\n",
    "        total_correct += int(((outputs > 0.0) == labels).sum())\n",
    "        total_samples += len(labels)\n",
    "    validation_accuracy = float(total_correct) / total_samples\n",
    "    if validation_accuracy > best[\"accuracy\"]:\n",
    "        best[\"epoch\"] = epoch + 1\n",
    "        best[\"accuracy\"] = validation_accuracy\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {loss.item():.6f} Training Accuracy: {training_accuracy:.6f} Validation Accuracy: {validation_accuracy:.6f}\")\n",
    "    torch.save(model.state_dict(), f\"alexnet_models/epoch_{epoch + 1}.pt\")\n",
    "\n",
    "print(f\"Best Validation Accuracy : {best['accuracy']} at Epoch {best['epoch']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09acf7c6190128958c2f4d29ef05b39ae6a25197494ca82b8d6c155c7139327c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
